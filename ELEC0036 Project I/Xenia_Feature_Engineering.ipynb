{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advance-cornell",
   "metadata": {},
   "source": [
    "# Part C1: Feature Engineering on the clean dataset\n",
    "\n",
    "This part aims to bring massive time-series data into reduced data by defining several statistical features representing the daily profiles, besides enriching the dataset by adding new dynamic influencing factors, like weather, day type, holidays that could integrate new knowledge. Therefore:\n",
    "\n",
    "1. **Data Segmentation**\n",
    "2. **Enriching the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-blogger",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selective-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy import stats\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "from calendar import day_abbr, month_abbr, mdays\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics import silhouette_score\n",
    "import holidays\n",
    "from calendar import day_abbr, month_abbr, mdays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-creativity",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "capable-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_trend(df,attribute):\n",
    "    hour_consumption =df.groupby(['hour']).mean()\n",
    "    q25 = df.groupby(['hour']).quantile(0.25)\n",
    "    q75 = df.groupby(['hour']).quantile(0.75)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10,7)) \n",
    "    hour_consumption.plot(ax=ax, lw=2, color='b', legend=False)\n",
    "    ax.fill_between(hour_consumption.index, q25.values.ravel(), q75.values.ravel(), color='b', alpha=0.3)\n",
    "    ax.grid(ls=':')\n",
    "    ax.set_xlabel('Hour', fontsize=15)\n",
    "    ax.set_ylabel( attribute, fontsize=15);\n",
    "    [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "    [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "    ax.set_title('Hourly ' + attribute + ' consumption of HH', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-sound",
   "metadata": {},
   "source": [
    "# 3. Import Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-guyana",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CleanDataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2a6897ef1074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../CleanDataset.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CleanDataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../CleanDataset.csv', index_col = 0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-underwear",
   "metadata": {},
   "source": [
    "# 4. Data Segmentation\n",
    "\n",
    "Time-series energy data often have high dimensions that can bring challenges to clustering algorithms based on the distance function (i.e., Euclidean distance), causing issues such as producing poor clustering results and increasing computational costs. Feature definition was therefore used for dimensionality reduction in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-comedy",
   "metadata": {},
   "source": [
    "## 4.1. Add time-scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dayofweek'] = df.index.dayofweek\n",
    "df['hour'] = df.index.hour\n",
    "df['month'] = df.index.month\n",
    "df['time'] = df.index.time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-extent",
   "metadata": {},
   "source": [
    "## 4.2. HH Electricity data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-hypothetical",
   "metadata": {},
   "source": [
    "Based on the hourly trend of Electricity data (same for HH and DE blocks), we divide the day into 5 segmentations, where for each segmentation we compute the <ins>mean</ins> value:\n",
    "1. 00:00 - 05:00 -> off-time\n",
    "2. 05:00 - 10:00 -> rise-time\n",
    "3. 10:00 - 15:00 -> day-time\n",
    "4. 15:00 - 22:00 -> evening\n",
    "5. 22:00 - 00:00 -> off-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HH_elec = df[['HH Electricity (kWh)', 'hour']]\n",
    "attribute = 'Electricity (kWh)'\n",
    "hourly_trend(df_HH_elec,attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-psychology",
   "metadata": {},
   "source": [
    "Besides the 5 new dimensions, we introduce a 6th feature, the daily peak-to-valley difference rate which is defined as the ratio of the differencce between the daily maximum and the minimum load to the daily maximum load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HH_elec = df[['HH Electricity (kWh)', 'dayofweek','hour','time']]\n",
    "df_HH_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-literacy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_HH_elec['Date'] = df.index.date\n",
    "#df_HH_elec['Time'] = pd.to_datetime(df.index,format='%H:%M:%S').time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HH_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_HH_elec_new = df_HH_elec.groupby('Date').resample(\"5H\").mean()['HH Electricity (kWh)']\n",
    "df_HH_elec_new = df_HH_elec.set_index(['Date',df_HH_elec.index])['HH Electricity (kWh)']\n",
    "df_HH_elec_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_func(s, h1, h2, h3, h4):\n",
    "    mean_list = []\n",
    "    # time segments = 22:00-05:00, 05:00-10:00, 10:00-15:00,15:00-22:00\n",
    "    sum_0 = 0\n",
    "    sum_1 = 0\n",
    "    sum_2 = 0\n",
    "    sum_3 = 0\n",
    "    min_ = 10000\n",
    "    max_ = 0\n",
    "    max_0 = 0\n",
    "    max_1 = 0\n",
    "    max_2 = 0\n",
    "    max_3 = 0\n",
    "    for i in range(0,s.shape[0]):\n",
    "\n",
    "        hh = s.index.get_level_values(1)[i].time().hour\n",
    "        if hh < h1:\n",
    "            sum_0 += s[i]\n",
    "            #print(\"1\",s[i])\n",
    "            if s[i] > max_0:\n",
    "                max_0 = s[i]\n",
    "        elif h1 <= hh < h2:\n",
    "            sum_1 += s[i] \n",
    "            #print(\"2\",s[i])\n",
    "            if s[i] > max_1:\n",
    "                max_1 = s[i]\n",
    "        elif h2<= hh < h3:\n",
    "            sum_2 += s[i]\n",
    "            #print(\"3\",s[i])\n",
    "            if s[i] > max_2:\n",
    "                max_2 = s[i]\n",
    "        elif h3 <= hh < h4:\n",
    "            sum_3 += s[i]\n",
    "            #print(\"4\",s[i])\n",
    "            if s[i] > max_3:\n",
    "                max_3 = s[i]\n",
    "        else: \n",
    "            sum_0 += s[i]\n",
    "            #print(\"5\",s[i])\n",
    "            if s[i] > max_0:\n",
    "                max_0 = s[i]\n",
    "        \n",
    "        #min\n",
    "        if s[i] < min_:\n",
    "            min_ = s[i]\n",
    "        #max\n",
    "        if s[i] > max_:\n",
    "            max_ = s[i]\n",
    "            \n",
    "    ## compute means for each segment    \n",
    "    mean_0 = round(sum_0/(2*(h1 + 24 - h4)),2)\n",
    "    mean_1 = round(sum_1/(2*(h2-h1)),2)\n",
    "    mean_2 = round(sum_2/(2*(h3-h2)),2)\n",
    "    mean_3 = round(sum_3/(2*(h4-h3)),2)\n",
    "    \n",
    "    ## compute paek-to-valley value = (max-min)/max\n",
    "    ptv = round((max_-abs(min_))/max_,2)\n",
    "    \n",
    "    stats_list = pd.Series([mean_0, mean_1, mean_2, mean_3, min_, max_, ptv])    \n",
    "    return stats_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-encoding",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define time segmentations\n",
    "h1=5\n",
    "h2=10\n",
    "h3=15\n",
    "h4=22\n",
    "df_HH_elec_new2 = df_HH_elec_new.groupby('Date').apply(lambda x: stats_func(x, h1, h2, h3, h4))\n",
    "df_HH_elec_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unstack multindex series to dataframe\n",
    "df_HH_elec_new2 = df_HH_elec_new2.unstack(level=1)\n",
    "df_HH_elec_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Elec_mean_0', 'Elec_mean_1', 'Elec_mean_2', 'Elec_mean_3',  'Elec_min_', 'Elec_max_', 'Elec_ptv']\n",
    "df_HH_elec = pd.DataFrame(df_HH_elec_new2)\n",
    "df_HH_elec.columns = col\n",
    "df_HH_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_HH_elec_new = df_HH_elec.groupby('Date').resample(\"5H\").mean()['HH Electricity (kWh)']\n",
    "# df_HH_elec_new = df_HH_elec_new.reset_index().rename(columns={'HH Electricity (kWh)':'Elec_mean'})\n",
    "# df_HH_elec_new = df_HH_elec_new.rename(columns={'From Timestamp':'Time'})\n",
    "# df_HH_elec_new['Time'] = df_HH_elec_new['Time'].dt.time\n",
    "#df_HH_elec_new = df_HH_elec_new.groupby(['Date','Time']).mean()\n",
    "# df_HH_elec_new = df_HH_elec_new.set_index(['Date','Time'])\n",
    "#df_HH_elec_new = df_HH_elec_new.unstack()\n",
    "#df_HH_elec_new1.columns = [col if type(col) is datetime else col for col in df_HH_elec_new.columns.values]\n",
    "# df_HH_elec_new = df_HH_elec_new.unstack().set_axis(['Elec_mean_00', 'Elec_mean_05', 'Elec_mean_10', 'Elec_mean_15', 'Elec_mean_20'], axis=1)\n",
    "# df_HH_elec_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-scoop",
   "metadata": {},
   "source": [
    "**Therefore we <ins>reduced a 48-dimension (data for every half an hour for each date) dataset to a 5-dimension dataset</ins>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-stranger",
   "metadata": {},
   "source": [
    "**Normalize data so  that all features belong to the same scale and Euclidean distance will not be biased in clustering methods:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = MinMaxScaler() #StandardScaler()\n",
    "df_HH_elec_scaled = scaler.fit_transform(df_HH_elec.values)\n",
    "df_HH_elec_scaled = pd.DataFrame(df_HH_elec_scaled, index=df_HH_elec.index, columns=df_HH_elec.columns)\n",
    "df_HH_elec_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = True: write row names (indexes)\n",
    "df_HH_elec_scaled.to_csv('../HH_elec_Dataset.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-advocate",
   "metadata": {},
   "source": [
    "## 4.2. HH Electricity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DE_elec = df[['DE Electricity (kWh)', 'hour']]\n",
    "attribute = 'Electricity (kWh)'\n",
    "hourly_trend(df_DE_elec,attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DE_elec = df[['DE Electricity (kWh)', 'dayofweek','hour','time']]\n",
    "df_DE_elec['Date'] = df.index.date\n",
    "df_DE_elec = df_DE_elec.set_index(['Date',df_DE_elec.index])['DE Electricity (kWh)']\n",
    "df_DE_elec = df_DE_elec.dropna(inplace=False)\n",
    "df_DE_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time segmentations\n",
    "h1=5\n",
    "h2=10\n",
    "h3=15\n",
    "h4=22\n",
    "df_DE_elec = df_DE_elec.groupby('Date').apply(lambda x: stats_func(x, h1, h2, h3, h4))\n",
    "#unstack multindex series to dataframe\n",
    "df_DE_elec = df_DE_elec.unstack(level=1)\n",
    "df_DE_elec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Elec_mean_0', 'Elec_mean_1', 'Elec_mean_2', 'Elec_mean_3',  'Elec_min_', 'Elec_max_', 'Elec_ptv']\n",
    "df_DE_elec = pd.DataFrame(df_DE_elec)\n",
    "df_DE_elec.columns = col\n",
    "\n",
    "# Standardize data\n",
    "scaler = MinMaxScaler() #StandardScaler()\n",
    "df_DE_elec_scaled = scaler.fit_transform(df_DE_elec.values)\n",
    "df_DE_elec_scaled = pd.DataFrame(df_DE_elec_scaled, index=df_DE_elec.index, columns=df_DE_elec.columns)\n",
    "df_DE_elec_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = True: write row names (indexes)\n",
    "df_DE_elec_scaled.to_csv('../DE_elec_Dataset.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-franklin",
   "metadata": {},
   "source": [
    "# 5. External data\n",
    "\n",
    "We enrich our dataset by adding external dynamic influencing factors for the energy consumption:\n",
    "\n",
    "- Weekday\n",
    "- Holiday\n",
    "- Month\n",
    "- Season\n",
    "- Mean_temp\n",
    "- Mean_hum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HH_elec_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-mirror",
   "metadata": {},
   "source": [
    "## 5.1. Weekday, Month, Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_of_date(date):\n",
    "    month = date.month\n",
    "    if 3<=month<=5:\n",
    "        return 'Spring'\n",
    "    elif 6<=month<=8:\n",
    "        return 'Summer'\n",
    "    elif 9<=month<=11:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-annual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_HH_elec_scaled['Weekday'] = df_HH_elec_scaled.index.map(lambda row: day_abbr[row.dayofweek])\n",
    "df_HH_elec_scaled['Month'] = df_HH_elec_scaled.index.map(lambda row: month_abbr[row.month])\n",
    "\n",
    "# season\n",
    "df_HH_elec_scaled['Season'] = df_HH_elec_scaled.index.map(lambda row: season_of_date(row))\n",
    "df_HH_elec_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-witness",
   "metadata": {},
   "source": [
    "## 5.2. The Holidays package\n",
    "\n",
    "Knowing when holidays and special events take place is often crucial when modelling time-series data. Here we make use of the `holidays` [package](https://github.com/dr-prodigy/python-holidays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_df = pd.DataFrame([], columns = ['Date','Holiday'])\n",
    "ldates = []\n",
    "lnames = []\n",
    "for date, name in sorted(holidays.England(years=np.arange(2018, 2019 + 1)).items()):\n",
    "    ldates.append(date)\n",
    "    lnames.append(name)\n",
    "    \n",
    "ldates = np.array(ldates)\n",
    "lnames = np.array(lnames)\n",
    "holidays_df.loc[:,'Date'] = ldates\n",
    "holidays_df.loc[:,'Holiday'] = lnames\n",
    "holidays_df.Holiday.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_df = holidays_df.set_index(['Date'])\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = pd.concat([df_HH_elec_scaled,holidays_df],axis=1)\n",
    "\n",
    "# Impute null value with new category (\"False\")\n",
    "df_enriched['Holiday'] = np.where(df_enriched['Holiday'].isnull(),\"False\",df_enriched['Holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-toner",
   "metadata": {},
   "source": [
    "## 5.3. Weather data\n",
    "\n",
    "Download averaged temp and hum for each date from [rp5.ru](https://rp5.ru/Weather_archive_in_Southampton_(airport),_METAR), acquired by the weather station located in Southampton (where Hursley House is located) from January 2018 to December 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('../Weather data-Southampton.xls')\n",
    "weather_df = pd.read_excel(xls, usecols = [0,1,4], parse_dates=True)\n",
    "weather_df = weather_df.rename(columns={weather_df.columns[0]: 'Timestamp',\n",
    "                                       weather_df.columns[1]: 'Temperature (C)',\n",
    "                                       weather_df.columns[2]: 'Humidity (%)'})\n",
    "weather_df=weather_df.set_index(['Timestamp'])\n",
    "weather_df.index = pd.to_datetime(weather_df.index)\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['Date'] = weather_df.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.set_index(['Date',weather_df.index])\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-korea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#weather_df[weather_df.index.get_level_values('Date').date == datetime.date(2018, 1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.groupby('Date').mean()\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = pd.concat([df_enriched, weather_df],axis=1)\n",
    "df_enriched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-rough",
   "metadata": {},
   "source": [
    "**NaN values:**\n",
    "\n",
    "(df_enriched had 730 rows, while df_weather had 728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.index[df_enriched['Temperature (C)'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-stanley",
   "metadata": {},
   "source": [
    "**Linear Interpolation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_df[weather_df.index.get_level_values('Date').date == datetime.date(2019, 12, 26)]\n",
    "df_enriched = df_enriched.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched[df_enriched.index.date == datetime.date(2019, 12, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = True: write row names (indexes)\n",
    "df_enriched.to_csv('../EnrichedDataset.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_col = 0: use 1st column as row labels\n",
    "# header = Row number(s) to use as the column names, and the start of the data.\n",
    "df_enriched = pd.read_csv('../EnrichedDataset.csv', index_col = 0) # header=[0,1]\n",
    "df_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-mozambique",
   "metadata": {},
   "source": [
    "# Part C2: Prepare daily time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-realtor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../ImputedDataset.csv', index_col = 0, parse_dates=True)\n",
    "df = df.drop(['dayofweek', 'hour','month'], axis=1)\n",
    "df1 = df.copy()\n",
    "\n",
    "# Scale data\n",
    "#scaler = MinMaxScaler() #better for using later sigmoind in NN\n",
    "#df1_scaled = scaler.fit_transform(df1.values)\n",
    "#df1 = pd.DataFrame(df1_scaled, index=df1.index, columns=df1.columns)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['hour'] = df1.index.hour\n",
    "df1['time'] = df1.index.time\n",
    "df1['Date'] = df1.index.date\n",
    "#df1 = df1.drop(df1.columns[6], axis=1) #drop hour column\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.set_index(['Date','time'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.unstack(level=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-recycling",
   "metadata": {},
   "source": [
    "**Add month, dayofweek attributes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['dayofweek'] = df1.index.dayofweek\n",
    "df1['month'] = df1.index.month\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-explorer",
   "metadata": {},
   "source": [
    "**Create 2 different datasets one for each building:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HH = df1.drop(['DE Electricity (kWh)', 'DE Boiler (kWh)'], axis=1)\n",
    "df_HH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DE = df1.drop(['HH Electricity (kWh)', 'HH Cooling (kWh)', 'HH Boiler (kWh)'], axis=1)\n",
    "df_DE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-african",
   "metadata": {},
   "source": [
    "**Convert categorical variables to one-hot-encoding vectors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek_dummies = pd.get_dummies(df1.dayofweek, prefix='Day')\n",
    "month_dummies = pd.get_dummies(df1.month, prefix='Month')\n",
    "df_HH = df_HH.drop(['month','dayofweek'], axis=1) #drop month, dayofweek columns\n",
    "df_HH = pd.concat([df_HH, dayofweek_dummies,month_dummies], axis=1)\n",
    "\n",
    "df_DE = df_DE.drop(['month','dayofweek'], axis=1) #drop month, dayofweek columns\n",
    "df_DE = pd.concat([df_DE, dayofweek_dummies,month_dummies], axis=1)\n",
    "\n",
    "\n",
    "df_HH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-hebrew",
   "metadata": {},
   "source": [
    "**Save datasets locally:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = True: write row names (indexes)\n",
    "df_HH.to_csv('../HH_Dataset.csv', index=True, header=True)\n",
    "df_DE.to_csv('../DE_Dataset.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_col = 0: use 1st column as row labels\n",
    "# header = Row number(s) to use as the column names, and the start of the data.\n",
    "df_imputed = pd.read_csv('../HH_Dataset.csv', index_col = 0) # header=[0,1]\n",
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc955650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
